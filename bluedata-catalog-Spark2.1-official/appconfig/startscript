#!/bin/bash
#
# Copyright 2016 (c) BlueData Software, Inc.
#
#

set -o pipefail
SELF=$(readlink -nf $0)
export CONFIG_BASE_DIR=$(dirname ${SELF})

source ${CONFIG_BASE_DIR}/logging.sh
source ${CONFIG_BASE_DIR}/utils.sh

if [[ "$1" == "--addnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--delnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--configure" ]]; then
    log "Starting configuration ... "

    ## Fall through to start the configuration.
else
    echo "ERROR: Unknown command line option(s): '$@'"
    exit 10
fi

# Indicate start of configuration to BD_VLIB
invoke_bdvcli --startconfiguration

source ${CONFIG_BASE_DIR}/macros.sh

# This macro automatically designates one node from the nodegroup as primary
AUTO_ASSIGN_PRIMARY

####################### AUTOGENERATED CODE STARTS BELOW #######################
#
[ ! -d '/usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/core-site.xml /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml

[ ! -d '/usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/' ] && mkdir -vp /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/
cp -rvf ${CONFIG_BASE_DIR}/zeppelin-site.xml /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-site.xml

[ ! -d '/usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/spark-defaults.conf /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-defaults.conf

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/spark-master /etc/init.d/spark-master

[ ! -d '/usr/bin' ] && mkdir -vp /usr/bin
cp -rvf ${CONFIG_BASE_DIR}/hadoop /usr/bin/hadoop

[ ! -d '/usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/spark-env.sh /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh

[ ! -d '/usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/' ] && mkdir -vp /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/
cp -rvf ${CONFIG_BASE_DIR}/zeppelin-env.sh /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/spark-slave /etc/init.d/spark-slave

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/zeppelin-server /etc/init.d/zeppelin-server

[ ! -d '/usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/' ] && mkdir -vp /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/
cp -rvf ${CONFIG_BASE_DIR}/zeppelin-log4j.properties /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-log4j.properties

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-defaults.conf GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@MASTER_HOST@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh GET_FQDN_LIST controller

REPLACE_PATTERN @@@@MEMORY@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh echo $(GET_TOTAL_VMEMORY_MB)m

REPLACE_PATTERN @@@@CORES@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh GET_TOTAL_VCORES

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@SPARK_HOME@@@ /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7

REPLACE_PATTERN @@@@ZEPPELIN_HOME@@@@ /etc/init.d/zeppelin-server echo /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT

REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-slave GET_NODE_FQDN

REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-master GET_FQDN_LIST controller

REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-slave echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7

REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-master echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7

REPLACE_PATTERN @@@@AWS_ACCESS_KEY@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml TENANT_INFO s3_access_key

REPLACE_PATTERN @@@@AWS_SECRET_KEY@@@@ /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml TENANT_INFO s3_secret_key

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /etc/init.d/spark-slave GET_SERVICE_URL spark_master controller


chmod 777 ${CONFIG_BASE_DIR}/total_vcores.sh
sh ${CONFIG_BASE_DIR}/total_vcores.sh && (echo ${CONFIG_BASE_DIR}/total_vcores.sh successful || exit 1)


REGISTER_START_SERVICE_SYSV spark_master spark-master
REGISTER_START_SERVICE_SYSV zeppelin-notebook zeppelin-server
REGISTER_START_SERVICE_SYSV spark_worker spark-slave
REGISTER_START_SERVICE_SYSV mysql mysqld
