#!/usr/bin/env bdwb
################################################################################
#                                                                              #
#  Sample workbench instructions for building a BlueData catalog entry.        #
#                                                                              #
################################################################################
#
# YOUR_ORGANIZATION_NAME must be replaced with a valid organization name. Please
# refer to 'help builder organization' for details.
#
builder organization --name BlueData

## Begin a new catalog entry
catalog new --distroid spark210-centos7 --name "Spark 2.1.0 Hadoop 2.7 Centos 7"                           \
            --desc "Apache Spark is general purpose  open source in memory     \
                    computational engine developed by the Apache Software      \
                    Foundation written in Scala."                              \
            --categories Spark --version 1.0

## Define all node roles for the virtual cluster.
role add controller 1
role add worker 0+

## Define all services that are available in the virtual cluster.
service add --srvcid spark --name "Spark master" --scheme "http" --port 8080   \
            --path "/" --display
service add --srvcid spark_master --name "Spark master" --scheme "spark"       \
            --port 7077 --export_as "spark"
service add --srvcid spark_worker --name "Spark worker" --scheme "http"        \
            --port 8081 --path "/" --display
service add --srvcid mysql --name "Mysql server" --port 3306
service add --srvcid zeppelin-notebook --name "Zeppelin server" --scheme "http" \
            --port 8055 --path "/" --display

## Define run time placement of the services
clusterconfig new --configid default
clusterconfig assign --configid default --role controller                  \
                     --srvcids spark spark_master spark_worker zeppelin-notebook mysql 

clusterconfig assign --configid default --role worker --srvcids spark_worker

## Appconfiguration autogenenration.
appconfig autogen --new --configapi 4

appconfig autogen --pkgfile spark-defaults.conf spark-env.sh   \
                  --destdir /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf

appconfig autogen --pkgfile core-site.xml --dest /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml

appconfig autogen --pkgfile hadoop --dest /usr/bin/hadoop

appconfig autogen --pkgfile spark-master spark-slave zeppelin-server 	 \
                  --destdir /etc/init.d/

appconfig autogen --pkgfile zeppelin-env.sh zeppelin-site.xml zeppelin-log4j.properties \
                  --destdir /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/

#replace pattern
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-defaults.conf \
                  --pattern @@@@SPARK_MASTER@@@@ --macro GET_SERVICE_URL spark_master controller
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@MASTER_HOST@@@@ --macro GET_FQDN_LIST controller
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@MEMORY@@@@ --macro echo "$(GET_TOTAL_VMEMORY_MB)m"
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@CORES@@@@ --macro GET_TOTAL_VCORES

## TODO : Need to convert below shell script to proper macro for replacing zeppelin_cores in
## the file zeppelin-env.sh
appconfig autogen --execute total_vcores.sh

appconfig autogen --replace /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh  \
                  --pattern @@@@SPARK_MASTER@@@@ --macro GET_SERVICE_URL spark_master controller

appconfig autogen --replace /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh  \
                  --pattern @@@SPARK_HOME@@@ --macro echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7

appconfig autogen --replace /etc/init.d/zeppelin-server --pattern @@@@ZEPPELIN_HOME@@@@               \
                  --macro "echo /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT"


appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@FQDN@@@@ --macro GET_NODE_FQDN
appconfig autogen --replace /etc/init.d/spark-master --pattern @@@@FQDN@@@@                   \
                  --macro GET_FQDN_LIST controller

appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@SPARK_HOME@@@@              \
                  --macro "echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7"
appconfig autogen --replace /etc/init.d/spark-master --pattern @@@@SPARK_HOME@@@@              \
                  --macro "echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7"

appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml --pattern @@@@AWS_ACCESS_KEY@@@@   \
                  --macro TENANT_INFO s3_access_key
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml --pattern @@@@AWS_SECRET_KEY@@@@   \
                  --macro TENANT_INFO s3_secret_key
appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@SPARK_MASTER@@@@                    \
                  --macro GET_SERVICE_URL spark_master controller

# The order of services defined here is the order in which they are brought up.
# The services defined here are only registered and started if the specific
# node is expected to run the service.
appconfig autogen --srvcid spark_master --sysv spark-master
appconfig autogen --srvcid spark_worker --sysv spark-slave
appconfig autogen --srvcid mysql --sysv mysqld
appconfig autogen --srvcid zeppelin-notebook --sysv zeppelin-server

appconfig autogen --generate
appconfig package

## Specify a logo file for the
logo file --filepath Logo_Spark.png

################################################################################
#                        Catalog package for CentOS                            #
################################################################################
image build --basedir image/centos --imgversion 1.0 --os centos
catalog save --filepath staging/spark210-centos.json --force
sources package
catalog package --os=centos

################################################################################
#                        Catalog package for RHEL                              #
#                                                                              #
# Set RHEL_USERNAME and RHEL_PASSWORD environment variables before invoking    #
# the workbench. The Dockerfile uses subscription-manager to subscribe before  #
# installing any packages. The subscription is cleaned up at the end of the    #
# build process in the same Dockerfile.                                        #
################################################################################
#image build --basedir image/rhel --imgversion 1.0 --os rhel
#catalog save --filepath staging/spark210-rhel.json --force
#sources package
#catalog package --os=rhel

workbench clean
